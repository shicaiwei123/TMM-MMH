


# 顺序:[accuracy, FAR, FRR, HTER, APCER, NPCER, ACER]
但是 FRR算错了,应该是FAR是FRR,而FRR是FAR
APCER 和NPCER 也反了

train multi modal
without random crop:
    test multimodal: [0.968758, 0.019105, 0.059228, 0.039166, 0.02552015203494829, 0.04472750538009655, 0.351]
    miss rgb: [0.942263, 0.040967, 0.096403, 0.068685, 0.041776299458869084, 0.09463957759412304, 6.78]
    miss ir: [0.713551, 2.5e-05, 0.946844, 0.473434, 0.29111850795160354, 0.001076426264800861]
    miss depth: [0.937758, 0.004422, 0.195555, 0.099989, 0.07850441501103753, 0.012515820559696245, 4.5]
    miss ir depth: [0.699463, 0.0, 0.99347, 0.496735, 0.30113202305715675, 0.0]
    miss rgb depth: [0.918333, 0.007528, 0.252606, 0.130067, 0.09941612750512861, 0.02269492921878511]
    miss rgb ir: [0.697574, 0.0, 0.999714, 0.499857, 0.302452127198683, 0.0]

    :[0.978531, 0.023477, 0.01684, 0.020159, 0.007424054948107371, 0.052183996907615]
    :[0.989066, 0.006832, 0.020392, 0.013612, 0.008826519227431632, 0.015825516487310815]





with rondom
    multi-modal:[0.912875, 0.119075, 0.013461, 0.066268, 0.006583739564072393, 0.21770530523255813]
    miss rgb:[0.810657, 0.260335, 0.025662, 0.142998, 0.01482412891697826, 0.3812070282658518]
    miss ir: [0.829718, 0.028098, 0.49811, 0.263104, 0.18186000794696447, 0.11432325886990802]
    miss depth:[0.951447, 0.006335, 0.145893, 0.076114, 0.059867431365174875, 0.01681392588685217]
    miss ir depth:[0.705666, 0.0, 0.972964, 0.486482, 0.2967608931129669, 0.0]
    miss rgb depth: [0.931034, 0.012596, 0.198935, 0.105765, 0.0803600351705308, 0.03498481921059895]
    miss ir rgb: [0.654635, 0.12931, 0.84351, 0.48641, 0.29586321901432505, 0.6557893410608542]








train single modality
depth without randorm crop: [0.861618, 0.001391, 0.454233, 0.227812, 0.1647757968665586, 0.005843071786310518]
depth with randorm crop:[0.910379, 0.112268, 0.037404, 0.074836, 0.01794646292530094, 0.21192084036766085]
ir with randorm crop:[0.940963, 0.035104, 0.114217, 0.07466, 0.04883305169838121, 0.08372341055874859]
ir without randorm crop:[0.940149, 0.022781, 0.14532, 0.084051, 0.0605894153611005, 0.05789872458643768]

rgb with randorm crop: [0.905666, 0.073015, 0.143487, 0.108251, 0.06291124617007383, 0.16426335792532976]

rgb withour random crop: [0.911107, 0.014012, 0.261542, 0.137777, 0.10317711393320378, 0.04191438763376932, 0.7], #或者就用原来的论文的

--epoch50
0:[0.917571, 0.036744, 0.187765, 0.112254, 0.077953, 0.09445]
1:[0.899983, 0.047501, 0.221102, 0.134301, 0.091469, 0.123275]
2:[0.901941, 0.029216, 0.256788, 0.143002, 0.102918, 0.083104]
3:[0.912684, 0.084915, 0.092851, 0.088883, 0.042153, 0.177512]


--epoch100

4:[0.905788, 0.062804, 0.166628, 0.114716, 0.071592, 0.148035]
5:[0.88061, 0.063425, 0.248425, 0.155925, 0.103173, 0.162881]
6:[0.890574, 0.056569, 0.231298, 0.143933, 0.096113, 0.14506]
7:[0.889031, 0.09043, 0.158323, 0.124377, 0.070195, 0.198538]


# 这样说来,还是epoch50比较合适.





rgb without maxpool
without random crop: [0.891942, 0.11823, 0.084603, 0.101417, 0.03995131187449283, 0.2294599807135969]
[0.878912, 0.117013, 0.130485, 0.123749, 0.060232681121099946, 0.2368024132730015],
[0.901837, 0.039178, 0.234162, 0.13667, 0.09559666066459323, 0.10550612162975848]
[0.873453, 0.136043, 0.104651, 0.120347, 0.04991394148020654, 0.25943999620978825],
[0.893467, 0.100219, 0.121091, 0.110655, 0.05514974433893353, 0.20817421818557127]
[0.914954, 0.090803, 0.071772, 0.081288, 0.03310435931307794, 0.1840382678751259]






miss modal

deeppix+share decoder
resnet18_se_no_dropout_no_seed_deeppix_3:
    without random:[0.876313, 0.050184, 0.293161, 0.171672, 0.11806228373702422, 0.14066852367688024



no shared decoder:
resnet18_se_no_dropout_no_seed_deeppix_4_rgb.pth:
without random: [0.891648, 0.05714, 0.226429, 0.141785, 0.0943324185658036, 0.14552356849098386]

resnet18_se_no_dropout_no_seed_deeppix_5
    without random:[0.903708, 0.04678, 0.210448, 0.128614, 0.08738672311680898, 0.12018893215037978]
resnet18_se_no_dropout_no_seed_deeppix_6
    without random:[0.921331, 0.047128, 0.151392, 0.09926, 0.06446655934435826, 0.11351124940162757]

resnet18_se_no_dropout_no_seed_deeppix_7
    without random:[0.920672, 0.068369, 0.104594, 0.086482, 0.04643238569902863, 0.1496953872932985]

resnet18_se_no_dropout_no_seed_deeppix_7
    without random: [0.874961, 0.162824, 0.03792, 0.100372, 0.019266589057043072, 0.28068522483940045],[0.926841, 0.054581, 0.115993, 0.085287, 0.050523952095808386, 0.12461712989222916],[0.909877, 0.047724, 0.187879, 0.117802, 0.07882531061498162, 0.11932418162618796]







enhanced deepix with binary with mutil-modal


without random: [0.868151, 0.033911, 0.357658, 0.195785, 0.1383527951962066, 0.10851419031719532],[0.911177, 0.066407, 0.140509, 0.103458, 0.0612759792166267, 0.15120488743070484],[0.907122, 0.040793, 0.212968, 0.126881, 0.08783783783783784, 0.10674814718502146]



enhanced deepix with binary
without random crop: [0.85467, 0.038806, 0.390938, 0.214872, 0.14995056574755575, 0.12808528085280854],

depth+pyramid
without random: [0.856784, 0.161358, 0.101386, 0.131372, 0.04982126270160723, 0.29279177748726504];  [0.885081, 0.055997, 0.250773, 0.153385, 0.10331319614876346, 0.1469936089735229],[0.865968, 0.122876, 0.159755, 0.141315, 0.07321170757317233, 0.2521539638032118]

depth+ir+pyramid+share decoder
without random: [0.896032, 0.078356, 0.16302, 0.120688, 0.07124974964950931, 0.17753011370032645]
              a: [0.89267, 0.09761, 0.12974, 0.113675, 0.058697004250025914, 0.205470139106788]
              b:[0.88787, 0.085834, 0.172757, 0.129296, 0.07575415065430889, 0.19304911437671118]

depth+ir+pyramid+noshare decoder
  resnet18_se_no_dropout_no_seed_pyramid_depth_ir_2
      without random: [0.916479, 0.064196, 0.128079, 0.096137, 0.05603448275862069, 0.1451196225991239]

  resnet18_se_no_dropout_no_seed_pyramid_depth_ir_3
      without random:[0.883019, 0.136316, 0.072402, 0.104359, 0.03508284992644814, 0.25307873253078733], [0.915924, 0.081412, 0.090217, 0.085814, 0.04085603112840467, 0.17103340292275573],[0.907936, 0.068742, 0.145836, 0.107289, 0.06360070945017611, 0.15651337745347588]

  resnet18_se_no_dropout_no_seed_pyramid_depth_ir_4
      without random:[0.919702, 0.06119, 0.124356, 0.092773, 0.05432932932932933, 0.1387605633802817],[0.914105, 0.087996, 0.081052, 0.084524, 0.03711475409836065, 0.18085269338779678],[0.919702, 0.06119, 0.124356, 0.092773, 0.05432932932932933, 0.1387605633802817]



cyclegan
without random: [0.662519, 0.258273, 0.520105, 0.389189, 0.2332032052599137, 0.5537445403217215]
with random: [0.585583, 0.480398, 0.262287, 0.371343, 0.179610888836589, 0.6002296995281847]

KD
without random:[0.92296, 0.063972, 0.107171, 0.085572, 0.047309598462627694, 0.14177953969827112]

baseline:

KD+multi_rgb
without random:[0.898406, 0.043575, 0.235365, 0.13947, 0.09643955218626048, 0.11613586704628219]

KD+multi_rgb+random init+logit
without random:[0.924935, 0.058208, 0.113931, 0.086069, 0.04985212291342925, 0.13154053447114306]

KD+multi_rgb+random init+no_maxpool-student
whitout radom: [0.920863, 0.039228, 0.171154, 0.105191, 0.07172175415856556, 0.09838619228612375],
[0.926235, 0.068767, 0.08529, 0.077029, 0.0382059374438714, 0.1477290921705716],
[0.91388, 0.061811, 0.14217, 0.10199, 0.06167072504099786, 0.14246449839670178]
[0.904869, 0.089089, 0.109062, 0.099075, 0.049364791288566245, 0.18735632183908046]



KD+multi_rgb+random init+patch + kl loss
without random:[[0.902807, 0.082605, 0.130828, 0.106716, 0.05824896075080972, 0.1797394453754257]
[0.900069, 0.107224, 0.083114, 0.095169, 0.03881028164870142, 0.21237022093194902]




KD+multi_rgb+random init+patch + bceloss
without random:[0.927482, 0.058407, 0.105052, 0.08173, 0.04615578205612181, 0.13079276773296244]
without random: [0.921158, 0.025489, 0.201856, 0.113673, 0.08243274853801169, 0.06858288770053476]   emmmmm+后面两个参数不错,但是前面的不行
without random: [0.934309, 0.037837, 0.129912, 0.083874, 0.05532112105763837, 0.09112666786333992]


KD+multi_rgb+random init+patch + bceloss + max_avg: epoch 50
[0.926581, 0.032868, 0.166915, 0.099891, 0.0696412781110341, 0.08338060124787294]
[0.932178, 0.043228, 0.124527, 0.083878, 0.05343361352799489, 0.10220864661654136]
[0.928435, 0.042234, 0.139191, 0.090713, 0.05929432433751403, 0.1016260162601626]
[0.933356, 0.039377, 0.129511, 0.084444, 0.05524335418295544, 0.09444643069955905]
[0.930965, 0.042532, 0.130141, 0.086336, 0.05566990100950701, 0.10131376494259675]


稳定:7.5±0.5

固定随机种子2,torch.mul_seed(2)
[0.927794, 0.030259, 0.16892, 0.09959, 0.0702427172903318, 0.07744642970687353]
[0.932195, 0.020943, 0.175851, 0.098397, 0.07227100449633937, 0.05534764624778413]
[0.932975, 0.032744, 0.146065, 0.089404, 0.061469482210008676, 0.08122765931221497]
[0.934933, 0.044793, 0.111811, 0.078302, 0.048315635751590306, 0.10416546305390259]

效果更好更稳定
KD+multi_rgb+random init+patch + bceloss + avg+ AT: epoch 50
不能使用max,因为有平方增强,使用max就去增强它去了
patch_kd_bce_avg_multi_multi_rgb_lr_0.001_version_0_weight_patch_True  multi_modal model without at
0:[0.943, 0.030837, 0.112853, 0.071845, 0.05172413793103448, 0.06907894736842106]
1:[0.930688, 0.044246, 0.127105, 0.085676, 0.054534283607766035, 0.10464159811985899]
2:[0.938659, 0.049091, 0.089586, 0.069339, 0.0392570281124498, 0.11057638500279798]
3:[0.943268, 0.024868, 0.130198, 0.077533, 0.05473942780078991, 0.061843568516001486]

:warmup
4:[0.940062, 0.032793, 0.122523, 0.077658, 0.05208054344914904, 0.07933169060640663, 6.65]
5:[0.943597, 0.036048, 0.103334, 0.069691, 0.04442802610515946, 0.08482899736919029, 6.4]
6:[0.927656, 0.053463, 0.115878, 0.084671, 0.0504199586272213, 0.12236310911468698, 8.51]
7:[0.938988, 0.04129, 0.106484, 0.073887, 0.04595910900145863, 0.09628642604715833, 7.02]
虽然有部分异常,但是整体是好的,而且着还是 multi_modal model没有增强的情况,不然效果可能更好?

0:[0.92568, 0.028694, 0.179517, 0.104105, 0.07421088773649688, 0.07461722333484075, 0.07441405553566882]
---best acer:[0.921417, 0.024794, 0.202601, 0.113697, 0.08265756818022481, 0.06689456397881896, 0.07477606607952189]
1:[0.931312, 0.043029, 0.12785, 0.085439, 0.054770318021201414, 0.1021346856940677, 0.07845250185763455]
---best acer:[0.925871, 0.024297, 0.189025, 0.106661, 0.07751209658477005, 0.06461416490486258, 0.07106313074481632]
2:
4:[0.924952, 0.028322, 0.182782, 0.105552, 0.07543200245845448, 0.07399234114363601, 0.07471217180104525]
5:[0.913932, 0.046159, 0.178085, 0.112122, 0.07491024745199142, 0.11464182143518233, 0.09477603444358687]
6:
emmmm, 感觉效果还差了呢?

# 取best 之后,效果不会更差,可能会好,可以试试.

KD+multi_rgb+random init+patch + mmd + avg+ AT: epoch 50
不能使用max,因为有平方增强,使用max就去增强它去了
# 好像鲁棒性更好了,不受初始化影响了,很不错.
重新测试
patch_kd_mmd_avg_multi_multi_rgb_lr_0.001_version_x_weight_patch_1.pth
warm up
0:[0.939664, 0.048693, 0.087181, 0.067937, 0.0382277590797207, 0.10952168082253018, 0.07387471995112543]
1:[0.933287, 0.050904, 0.103162, 0.077033, 0.04502049795020498, 0.11572348356489326, 0.08037199075754911]
2:[0.937706, 0.052693, 0.084431, 0.068562, 0.0372175230400202, 0.11714995857497928, 0.07718374080749973]
3:[0.932126, 0.062208, 0.080937, 0.071573, 0.03608181609254105, 0.1349938002048628, 0.08553780814870192]

analysis: 其中一个分析不好的,版本1,是因为刚好那个节点没有采到好的hter和acer,可以通过best_hter,acer 解决.









patch_kd_mmd_avg_spp_multi_multi_rgb_lr_0.001_version_0_weight_patch_1.pth
： SPP with AT
warm up
0:[0.94053, 0.045414, 0.091878, 0.068646, 0.04007194963525532, 0.10338197036534329, 0.07172696000029931]
1:[0.939854, 0.048768, 0.086379, 0.067573, 0.03789230343995779, 0.10958521743984816, 0.07373876043990298]
2:[0.937792, 0.057562, 0.072918, 0.06524, 0.03246786370128545, 0.1252297048967679, 0.07884878429902668]
3:[0.930445, 0.065786, 0.078245, 0.072015, 0.035052604567616115, 0.14130202774813233, 0.08817731615787422]


analysis: 基本上是不分上下，甚至还比普通的直接展开最后的特征要好










patch_kd_mmd_avg_spp_multi_multi_rgb_lr_0.001_version_0_weight_patch_0.pth

SPP without AT
0:[0.943164, 0.040694, 0.094054, 0.067374, 0.0407889507154213, 0.09384668270883464, 0.06731781671212797]
1:[0.938693, 0.046557, 0.095314, 0.070936, 0.04155636581589331, 0.10606746660629386, 0.07381191621109359]
2:[0.937498, 0.044942, 0.10299, 0.073966, 0.04468079819089983, 0.1035548686244204, 0.07411783340766012]







patch_kd_bce_avg_spp_multi_multi_rgb_lr_0.001_version_0_weight_patch_0.pth

SPP without AT
0:[0.94202, 0.032545, 0.116623, 0.074584, 0.0496851969349407, 0.07829309108295482, 0.06398914400894776]
1:[0.937481, 0.041364, 0.111296, 0.07633, 0.047939797680730326, 0.09691501746216531, 0.07242740757144782]
2:[0.939854, 0.035228, 0.117597, 0.076412, 0.05021155868613496, 0.08428936574927183, 0.0672504622177034]
3:[0.931208, 0.058357, 0.092851, 0.075604, 0.041013055358769354, 0.12916529198284393, 0.08508917367080665]

# 这样的话,感觉bceloss 就不行了,mmd 永远的神.









KD+multi_rgb+random init+patch +mmd loss  :比hter可以考虑这个,但是比acer就算了,emmmm,没有特别的优势感觉.
without random:
[0.933218, 0.060419, 0.081453, 0.070936, 0.03623668518424138, 0.13168724279835392],
[0.929284, 0.03411, 0.155115, 0.094613, 0.06511650275326424, 0.08515784903553929]
[0.912857, 0.098952, 0.059915, 0.079433, 0.02803162267184778, 0.19529296396175533]

KD+multi_rgb+random init+patch +mmd loss+max_avg  : epoch 50?
[0.935904, 0.053612, 0.088269, 0.070941, 0.03887977797401287, 0.11939142461964039]
[0.916877, 0.080592, 0.088956, 0.084774, 0.04027385181919556, 0.1694083241944749]
[0.926529, 0.073835, 0.072631, 0.073233, 0.0328940541662343, 0.155098632710573]
[0.933894, 0.05155, 0.099668, 0.075609, 0.043590450184132074, 0.11661889507109538]

patch_kd_mmd_max_avg_multi_multi_rgb_lr_0.001_version_1_sift_0
[0.935488, 0.048047, 0.102475, 0.075261, 0.04460567980651756, 0.10986763619837528]
[0.93424, 0.042805, 0.118685, 0.080745, 0.05103322578261619, 0.10070723011280612]


KD+multi_rgb+random init+patch +mmd+kl loss+max_avg  :
[0.929094, 0.046681, 0.126761, 0.086721, 0.05452619129749175, 0.10972903527213268]
[0.920256, 0.069984, 0.102245, 0.086115, 0.04551249362570117, 0.1523526230394808]
效果反而差了

mmd,3,10 效果都不怎么好,不知道其他咋么样


KD+multi_rgb+random init+sift_patch +mmd loss+max_avg  :
[0.930116, 0.046408, 0.124012, 0.08521, 0.05339219216256874, 0.10885146553231163]
[0.935748, 0.040545, 0.118914, 0.079729, 0.0510123845095341, 0.09592100623016339]
[0.931381, 0.046333, 0.120002, 0.083168, 0.05175139568203152, 0.10825400510796378]
总的来说,mmd的优点并没有得到增强


KD+multi_rgb+random init+sift_patch +bce loss+max_avg  :
[0.934743, 0.044321, 0.11353, 0.078925, 0.04899876390605686, 0.1033603707995365]
[0.935748, 0.040545, 0.118914, 0.079729, 0.0510123845095341, 0.09592100623016339]
[0.917935, 0.066307, 0.118398, 0.092353, 0.05213114754098361, 0.14778516057585825]
[0.92568, 0.051327, 0.127334, 0.08933, 0.05501249721596674, 0.11941506271313797]
[0.937082, 0.036247, 0.124413, 0.08033, 0.05302087147564995, 0.08713048671245148]

总的来说,bce效果好像有改善?
但是不稳定

KD+multi_rgb+random init+sift_patch +bce loss+avg  :
[0.935418, 0.048345, 0.102016, 0.075181, 0.044428368298949784, 0.11042387788685241]
[0.935089, 0.049066, 0.101443, 0.075255, 0.0442219336795845, 0.11182199071452836]
[0.939283, 0.050532, 0.084202, 0.067367, 0.03703890344688571, 0.11286205748529575]
[0.91284, 0.077785, 0.108775, 0.09328, 0.04866735007688365, 0.16752273943285179]


#new
[0.931936, 0.040122, 0.132489, 0.086306, 0.05648351648351648, 0.09636038186157518]


去掉max之后好像效果有回升? 不稳定


KD+multi_rgb+random init+sift_patch +bce loss+avg without droppout:
[0.898891, 0.102156, 0.098694, 0.100425, 0.04550616697039326, 0.20718496498211317]

验证高,但是测试很低....


KD+multi_rgb+random init+sift_patch +weight-bce loss+max_avg  :
[0.927188, 0.056345, 0.11078, 0.083563, 0.04844932110827196, 0.12747302158273383]



KD+multi_rgb+random init+patch+no maxpool-student+bceloss
without random:[0.94131, 0.040371, 0.100928, 0.070649, 0.04362573968159648, 0.09381675422897061],
[0.937706, 0.054507, 0.08025, 0.067378, 0.03550520793735269, 0.12021259109089913],
[0.925212, 0.072767, 0.079448, 0.076107, 0.03583053474554379, 0.1541578947368421]





KD+multi_rgb+random init+logit+feature+dropout:
[0.923098, 0.046905, 0.146065, 0.096485, 0.062325854230825636, 0.1124077161228864]
[0.899394, 0.073139, 0.163936, 0.118538, 0.07124719940253921, 0.1678449258836944]
[0.910917, 0.043551, 0.194066, 0.118808, 0.08088428390670137, 0.11078809328193137]
效果很差,甚至比单独logits kd还差


KD+multi_rgb+random init+feature+dropout:
[0.927205, 0.021365, 0.191374, 0.106369, 0.07818313715395596, 0.05742137944848768]
----[0.930913, 0.043725, 0.127563, 0.085644, 0.05469191286622952, 0.10358425048555117]
这,不同终止条件差的这么多吗???不以准确率来选择模型呢?

[0.931138, 0.038035, 0.139936, 0.088986, 0.059347973957827226, 0.09252991659615617],
[0.929059, 0.02775, 0.170524, 0.099137, 0.07069243920972644, 0.07161174509552506]


KD+multi_rgb+random init+patch + bceloss + avg + feature:
[0.926027, 0.025514, 0.185703, 0.105609, 0.07634162997150729, 0.06737518861116579]
mse


KD+multi_rgb+random init+patch + mmd + max_avg + feature:

[0.926581, 0.047873, 0.132318, 0.090095, 0.056847545219638244, 0.11285505124450952]

# seed_torch(5)
[0.93398, 0.02698, 0.156032, 0.091506, 0.06502745285270947, 0.0686472819216182]
[0.942072, 0.033166, 0.115019, 0.074092, 0.04906536346976176, 0.07953529937444147]

# only torch.multu_seed(5)
[0.939491, 0.039476, 0.109004, 0.07424, 0.04691120642902924, 0.09268548763415772, 6.9]
[0.935349, 0.039973, 0.121549, 0.080761, 0.052054458481540535, 0.09495426379462968, 7.3]










KD+multi_rgb+random init+logit+feature+dropout+ attention(all):
[0.928331, 0.048022, 0.126189, 0.087106, 0.054365529835644834, 0.1124621829183151, 0.08341385637697997]
[0.923185, 0.058755, 0.118456, 0.088605, 0.05175822800650732, 0.13320191495353423, 0.09248007148002077]
[0.936701, 0.030905, 0.137988, 0.084447, 0.058164521814713765, 0.07635180752470386, 0.06725816466970881]
[0.924173, 0.050532, 0.134151, 0.092341, 0.05774161735700197, 0.118600583090379, 0.08817110022369049]


KD+multi_rgb+random init+logit+feature+dropout+ attention(single):
[0.884301, 0.121982, 0.101214, 0.111598, 0.04761648117707295, 0.23833794475996312, 0.14297721296851804]
[0.900191, 0.090232, 0.121893, 0.106062, 0.05491896355940952, 0.19154097669022255, 0.12322997012481604]
[0.928279, 0.034806, 0.156834, 0.09582, 0.06583471591045709, 0.08690527882885678, 0.07636999736965694]
[0.915561, 0.073413, 0.109864, 0.091638, 0.048909855922478644, 0.1597729115977291, 0.10434138376010388]

[0.918922, 0.078481, 0.087066, 0.082773, 0.03936498070597985, 0.16541865214431586, 0.10239181642514786]


KD+multi_rgb+random init+logit+sp_feature+dropout+ attention(all):
warm up
[0.931572, 0.045637, 0.120976, 0.083307, 0.05211340587756311, 0.10690799045568294, 0.07951069816662303]
[0.930445, 0.046383, 0.122981, 0.084682, 0.05297049245040955, 0.10868552800093143, 0.08082801022567049]
如果结合feature 那么都加attention会好?










###########################spp+multi-kd#####################
multi-kd
patch_kd_multiKD_avg_spp_multi_multi_rgb_lr_0.001_version_0_weight_patch_0

witout weight_patch
0:[0.937203, 0.046855, 0.099553, 0.073204, 0.04333732296030321, 0.10712257185050551, 0.07522994740540437]
acer_best:[0.933894, 0.029365, 0.150819, 0.090092, 0.06313694458432247, 0.07384269382145311, 0.0684898192028878]
1:[0.933547, 0.040818, 0.125558, 0.083188, 0.05372417342712189, 0.09716718907090899, 0.07544568124901543]

2:[0.929874, 0.025365, 0.17333, 0.099348, 0.07160943748964668, 0.0660713130136543, 0.06884037525165049]

3:[0.942194, 0.04278, 0.09245, 0.067615, 0.04020526106018334, 0.09803028577934647, 0.06911777341976491]


74.33   73.21

15636 1822 38242 2010
[0.933599, 0.049935, 0.104365, 0.07715, 0.04547723642172524, 0.1139068344100646, 0.07969203541589492]
16091 1367 37639 2613
[0.933668, 0.055774, 0.090675, 0.073224, 0.03998484465774185, 0.1238962472406181, 0.08194054594917997]
15471 1987 38427 1825
[0.933946, 0.045339, 0.113816, 0.079578, 0.049166130548819714, 0.10551572617946346, 0.07734092836414158]
15748 1710 38328 1924
[0.93703, 0.047799, 0.097949, 0.072874, 0.04270942604525701, 0.1088727931190584, 0.0757911095821577]
15841 1617 38131 2121
[0.935228, 0.052693, 0.092622, 0.072658, 0.040681292140485055, 0.11808261886204209, 0.07938195550126358]
15683 1775 38009 2243
[0.930376, 0.055724, 0.101673, 0.078698, 0.04461592600040217, 0.12512551601026442, 0.0848707210053333]


75.1   79.1





acer_best:
15624 1834 38484 1768
[0.937584, 0.043923, 0.105052, 0.074488, 0.045488367478545566, 0.10165593376264949, 0.07357215062059752]
14825 2633 39070 1182
[0.933894, 0.029365, 0.150819, 0.090092, 0.06313694458432247, 0.07384269382145311, 0.0684898192028878]
14432 3026 39231 1021
[0.929874, 0.025365, 0.17333, 0.099348, 0.07160943748964668, 0.0660713130136543, 0.06884037525165049]
15844 1614 38530 1722
[0.942194, 0.04278, 0.09245, 0.067615, 0.04020526106018334, 0.09803028577934647, 0.06911777341976491]
12166 5292 39805 447

:weight
[0.900554, 0.011105, 0.303128, 0.157116, 0.11734705191032663, 0.03543962578292238, 0.0763933388466245]
12660 4798 39749 503
[0.908144, 0.012496, 0.274831, 0.143664, 0.10770646732664377, 0.03821317328876396, 0.07295982030770387]
14331 3127 39375 877
[0.930619, 0.021788, 0.179116, 0.100452, 0.0735730083290198, 0.05766701735928459, 0.0656200128441522]
14167 3291 39386 866
[0.927967, 0.021514, 0.18851, 0.105012, 0.07711413642008576, 0.05760659881593827, 0.06736036761801201]

所以weight_best 不可取.


new_processing----也就是,恩,转sigmoid 为一般的kl,multi-kl

15371 2087 38594 1658
[0.935107, 0.04119, 0.119544, 0.080367, 0.05130159042304761, 0.09736332139291796, 0.07433245590798279]
14980 2478 38818 1434
[0.932213, 0.035626, 0.141941, 0.088783, 0.06000581170089113, 0.08736444498598757, 0.07368512834343935]
15502 1956 38405 1847
[0.934102, 0.045886, 0.11204, 0.078963, 0.048462624811080005, 0.10646146751974177, 0.07746204616541089]
15520 1938 38688 1564
[0.939317, 0.038855, 0.111009, 0.074932, 0.0477034411460641, 0.09154764692109577, 0.06962554403357993]
15830 1628 37866 2386
[0.930445, 0.059277, 0.093252, 0.076264, 0.041221451359700206, 0.13098375054896794, 0.08610260095433407]













nulti-kd-new
patch_kd_multiKD_new_multi_multi_rgb_lr_0.001_version_" + str(i) + "_weight_patch_0_select_0.pth"

[0.928175, 0.068394, 0.079734, 0.074064, 0.03579234270139621, 0.1462883256283543, 0.09104033416487525]
15985 1473 37400 2852
[0.925056, 0.070854, 0.084374, 0.077614, 0.03789262470094924, 0.1514041514041514, 0.09464838805255033]
16104 1354 37050 3202
[0.921054, 0.079549, 0.077558, 0.078553, 0.03525674408915738, 0.1658551745571325, 0.10055595932314494]
15417 2041 38732 1520
[0.938295, 0.037762, 0.116909, 0.077336, 0.05005763618080593, 0.08974434669658145, 0.06990099143869369]
15759 1699 37620 2632
[0.924952, 0.065388, 0.097319, 0.081354, 0.043210661512245986, 0.14311347941928118, 0.09316207046576358]
16135 1323 37712 2540
[0.933062, 0.063102, 0.075782, 0.069442, 0.033892660432944796, 0.1360107095046854, 0.08495168496881511]
15622 1836 38101 2151
[0.930913, 0.053438, 0.105167, 0.079303, 0.04597240654030097, 0.12102627581162437, 0.08349934117596267]




multikd-spp_max


select_0
15924 1534 37697 2555
[0.929146, 0.063475, 0.087868, 0.075672, 0.039101730774132704, 0.1382650576329888, 0.08868339420356075]
14208 3250 38280 1972
[0.909513, 0.048991, 0.186161, 0.117576, 0.07825668191668674, 0.12187886279357231, 0.10006777235512952]
16047 1411 37803 2449
[0.933114, 0.060842, 0.080823, 0.070832, 0.035982047228030806, 0.13240700692041524, 0.08419452707422302]
15360 2098 38488 1764
[0.933079, 0.043824, 0.120174, 0.081999, 0.05169270191691716, 0.10301331464611072, 0.07735300828151394]
13638 3820 38850 1402
[0.909513, 0.034831, 0.218811, 0.126821, 0.08952425591750644, 0.09321808510638298, 0.09137117051194471]
15929 1529 37673 2579
[0.928816, 0.064071, 0.087582, 0.075826, 0.03900311208611806, 0.13934514804408904, 0.08917413006510355]
15977 1481 37230 3022
[0.921972, 0.075077, 0.084832, 0.079955, 0.03825785952313296, 0.1590610032106953, 0.09865943136691413]
16091 1367 37627 2625
[0.930827, 0.065214, 0.078302, 0.071758, 0.03505667538595681, 0.14025432784783073, 0.08765550161689377]




select_1
15892 1566 37712 2540
[0.928851, 0.063102, 0.089701, 0.076402, 0.039869647130709306, 0.13780381944444445, 0.08883673328757688]
16096 1362 37495 2757
[0.928626, 0.068493, 0.078016, 0.073255, 0.035051599454409756, 0.14623667320850792, 0.09064413633145883]
15864 1594 37898 2354
[0.931589, 0.058482, 0.091305, 0.074893, 0.040362605084574094, 0.1292128663958722, 0.08478773574022315]
15868 1590 37657 2595
[0.927482, 0.064469, 0.091076, 0.077772, 0.04051265064845721, 0.14055137301630288, 0.09053201183238005]








multikd_spp_avg:
sleclt_0
15219 2239 38716 1536
[0.934587, 0.03816, 0.128251, 0.083205, 0.054669759492125505, 0.09167412712623098, 0.07317194330917824]
16335 1123 36346 3906
[0.912857, 0.097039, 0.064326, 0.080682, 0.029971443059595933, 0.19297465540240108, 0.1114730492309985]
16386 1072 36529 3723
[0.916912, 0.092492, 0.061405, 0.076948, 0.02850988005638148, 0.18514098165000745, 0.10682543085319446]
15893 1565 37817 2435
[0.930688, 0.060494, 0.089644, 0.075069, 0.03973896704078005, 0.13285683107813182, 0.08629789905945594]
13115 4343 39308 944
[0.908387, 0.023452, 0.248768, 0.13611, 0.09949371148427298, 0.06714560068283662, 0.0833196560835548]
12081 5377 39779 473
[0.898631, 0.011751, 0.307996, 0.159874, 0.11907609177075029, 0.03767723434761829, 0.0783766630591843]
15067 2391 38027 2225
[0.920014, 0.055277, 0.136957, 0.096117, 0.05915681132168836, 0.12867221836687487, 0.09391451484428162]
15193 2265 38412 1840
[0.928868, 0.045712, 0.12974, 0.087726, 0.05568257246109595, 0.10802559736981154, 0.08185408491545375]


select_1
15865 1593 37733 2519
[0.928747, 0.062581, 0.091248, 0.076914, 0.040507552255505265, 0.1370213228894691, 0.08876443757248718]
15060 2398 38780 1472
[0.932941, 0.03657, 0.137358, 0.086964, 0.05823497984360581, 0.08903943866440842, 0.07363720925400712]
16011 1447 36585 3667
[0.911385, 0.091101, 0.082885, 0.086993, 0.03804690786705932, 0.18635023884541113, 0.11219857335623523]
16091 1367 36614 3638
[0.913273, 0.090381, 0.078302, 0.084341, 0.03599168005055159, 0.1843986010441482, 0.1101951405473499]
16086 1372 37707 2545
[0.932126, 0.063227, 0.078589, 0.070908, 0.03510837022441721, 0.13660028983951478, 0.08585433003196599]
12410 5048 39466 786
[0.898908, 0.019527, 0.289151, 0.154339, 0.11340252504829941, 0.05956350409214914, 0.08648301457022428]
14849 2609 38736 1516
[0.928522, 0.037663, 0.149444, 0.093554, 0.06310315636715443, 0.09263672471738467, 0.07786994054226955]
16179 1279 36764 3488
[0.917397, 0.086654, 0.073262, 0.079958, 0.03361985122098678, 0.17735292622158946, 0.10548638872128813]






warm_5
#surf_patch_kd_multikd_avg_multi_multi_rgb_lr_0.001_version_" + str(i) + "_weight_patch_0_select_0_acer_best_.pth
15755 1703 38055 2197
[0.932421, 0.054581, 0.097548, 0.076065, 0.04283414658685045, 0.12238190730837789, 0.08260802694761417]
15804 1654 37936 2316
[0.931208, 0.057538, 0.094742, 0.07614, 0.0417782268249558, 0.12781456953642384, 0.08479639818068982]
15266 2192 38303 1949
[0.928245, 0.04842, 0.125558, 0.086989, 0.05413013952339795, 0.11321521928550683, 0.08367267940445239]
15194 2264 38905 1347
[0.937429, 0.033464, 0.129683, 0.081573, 0.05499283441424373, 0.08143401245390243, 0.06821342343407308]
14869 2589 38861 1391
[0.931034, 0.034557, 0.148299, 0.091428, 0.062460796139927624, 0.08554735547355473, 0.07400407580674118]






old+select_1
15615 1843 37996 2256
[0.928972, 0.056047, 0.105568, 0.080807, 0.04626120133537488, 0.12623803928151756, 0.08624962030844621]
15956 1502 37674 2578
[0.929302, 0.064047, 0.086035, 0.075041, 0.038339799877476, 0.13909571598143952, 0.08871775792945777]
15118 2340 38014 2238
[0.920672, 0.0556, 0.134036, 0.094818, 0.05798681667244883, 0.12894676192671123, 0.09346678929958002]
14428 3030 38465 1787
[0.916531, 0.044395, 0.173559, 0.108977, 0.07302084588504638, 0.11020659882824545, 0.09161372235664592]
15362 2096 38363 1889
[0.930948, 0.046929, 0.12006, 0.083494, 0.05180553152574211, 0.10950089849863776, 0.08065321501218993]










acer_best:
12521 4937 39624 628
[0.90357, 0.015602, 0.282793, 0.149197, 0.11079194811606562, 0.047760285953304435, 0.07927611703468503]
16066 1392 37499 2753
[0.928175, 0.068394, 0.079734, 0.074064, 0.03579234270139621, 0.1462883256283543, 0.09104033416487525]
11405 6053 39550 702
[0.882949, 0.01744, 0.346718, 0.182079, 0.13273249566914458, 0.05798298504997109, 0.09535774035955784]
16104 1354 37050 3202
[0.921054, 0.079549, 0.077558, 0.078553, 0.03525674408915738, 0.1658551745571325, 0.10055595932314494]
14774 2684 39236 1016
[0.935886, 0.025241, 0.15374, 0.089491, 0.06402671755725191, 0.0643445218492717, 0.06418561970326181]
15759 1699 37620 2632
[0.924952, 0.065388, 0.097319, 0.081354, 0.043210661512245986, 0.14311347941928118, 0.09316207046576358]
16135 1323 37712 2540
[0.933062, 0.063102, 0.075782, 0.069442, 0.033892660432944796, 0.1360107095046854, 0.08495168496881511]
15622 1836 38101 2151
[0.930913, 0.053438, 0.105167, 0.079303, 0.04597240654030097, 0.12102627581162437, 0.08349934117596267]





16094 1364 36500 3752
[0.91135, 0.093213, 0.07813, 0.085672, 0.03602366363828439, 0.18905572911417917, 0.11253969637623178]
16520 938 35980 4272
[0.909721, 0.106131, 0.053729, 0.07993, 0.02540766021994691, 0.2054636398614852, 0.11543565004071606]
15985 1473 37400 2852
[0.925056, 0.070854, 0.084374, 0.077614, 0.03789262470094924, 0.1514041514041514, 0.09464838805255033]
16167 1291 36358 3894
[0.910154, 0.096741, 0.073949, 0.085345, 0.034290419400249676, 0.19410797068939734, 0.11419919504482351]
15706 1752 38156 2096
[0.933322, 0.052072, 0.100355, 0.076214, 0.04390097223614313, 0.11773957982249185, 0.08082027602931749]
15759 1699 37620 2632
[0.924952, 0.065388, 0.097319, 0.081354, 0.043210661512245986, 0.14311347941928118, 0.09316207046576358]
16135 1323 37712 2540
[0.933062, 0.063102, 0.075782, 0.069442, 0.033892660432944796, 0.1360107095046854, 0.08495168496881511]
16391 1067 36281 3971
[0.912701, 0.098653, 0.061118, 0.079886, 0.02856913355467495, 0.1950201355466064, 0.11179463455064068]






multi-kd new select
patch_kd_multiKD_new_multi_multi_rgb_lr_0.001_version_" + str(i) + "_weight_patch_0_select_1_hter_best_.pth"

16343 1115 36839 3413
[0.921539, 0.084791, 0.063868, 0.074329, 0.029377667703008904, 0.17275764324762097, 0.10106765547531493]
16016 1442 38049 2203
[0.936839, 0.05473, 0.082598, 0.068664, 0.03651464890734598, 0.1209177232559416, 0.0787161860816438]
15601 1857 38100 2152
[0.930532, 0.053463, 0.10637, 0.079916, 0.046474960582626325, 0.12121894891004337, 0.08384695474633484]
15034 2424 38498 1754
[0.927604, 0.043575, 0.138848, 0.091211, 0.05923464151312253, 0.10447939004050512, 0.08185701577681383]
15188 2270 38797 1455
[0.935453, 0.036147, 0.130026, 0.083087, 0.05527552536099545, 0.0874241422820405, 0.07134983382151797]
15761 1697 38285 1967
[0.93651, 0.048867, 0.097205, 0.073036, 0.04244409984493022, 0.11095442238267147, 0.07669926111380085]
16086 1372 37537 2715
[0.92918, 0.06745, 0.078589, 0.073019, 0.035261764630291195, 0.1444072123823201, 0.08983448850630565]
16163 1295 37503 2749
[0.929925, 0.068295, 0.074178, 0.071236, 0.03337800917573071, 0.14535744500846023, 0.08936772709209548]


acer_best

16343 1115 36839 3413
[0.921539, 0.084791, 0.063868, 0.074329, 0.029377667703008904, 0.17275764324762097, 0.10106765547531493]
16016 1442 38049 2203
[0.936839, 0.05473, 0.082598, 0.068664, 0.03651464890734598, 0.1209177232559416, 0.0787161860816438]
12990 4468 39150 1102
[0.903483, 0.027378, 0.255929, 0.141653, 0.10243477463432528, 0.07820039738858928, 0.09031758601145728]
15034 2424 38498 1754
[0.927604, 0.043575, 0.138848, 0.091211, 0.05923464151312253, 0.10447939004050512, 0.08185701577681383]
15188 2270 38797 1455
[0.935453, 0.036147, 0.130026, 0.083087, 0.05527552536099545, 0.0874241422820405, 0.07134983382151797]
15376 2082 38775 1477
[0.93833, 0.036694, 0.119258, 0.077976, 0.05095822013363683, 0.0876401827567792, 0.06929920144520801]
16086 1372 37537 2715
[0.92918, 0.06745, 0.078589, 0.073019, 0.035261764630291195, 0.1444072123823201, 0.08983448850630565]
16163 1295 37503 2749
[0.929925, 0.068295, 0.074178, 0.071236, 0.03337800917573071, 0.14535744500846023, 0.08936772709209548]







hter_best


[0.912095, 0.101784, 0.055906, 0.078845, 0.0262853141579812, 0.19908644734923953, 0.11268588075361036]
16148 1310 37815 2437
[0.935072, 0.060544, 0.075037, 0.06779, 0.03348242811501598, 0.13112725316115145, 0.08230484063808371]
15898 1560 37799 2453
[0.930463, 0.060941, 0.089357, 0.075149, 0.03963515333214766, 0.13367118958094926, 0.08665317145654847]
16410 1048 36086 4166
[0.909652, 0.103498, 0.06003, 0.081764, 0.02822211450422793, 0.20246889580093314, 0.11534550515258053]
15659 1799 38443 1809
[0.937481, 0.044942, 0.103047, 0.073995, 0.04470453754783559, 0.10356079688573391, 0.07413266721678476]
15761 1697 38285 1967
[0.93651, 0.048867, 0.097205, 0.073036, 0.04244409984493022, 0.11095442238267147, 0.07669926111380085]
16086 1372 37537 2715
[0.92918, 0.06745, 0.078589, 0.073019, 0.035261764630291195, 0.1444072123823201, 0.08983448850630565]
16163 1295 37503 2749
[0.929925, 0.068295, 0.074178, 0.071236, 0.03337800917573071, 0.14535744500846023, 0.08936772709209548]




student + linear + multikd

15628 1830 38087 2165
[0.930775, 0.053786, 0.104823, 0.079305, 0.045845128641932006, 0.12167706401393806, 0.08376109632793503]
15286 2172 38584 1668
[0.93346, 0.041439, 0.124413, 0.082926, 0.05329276670919619, 0.09838386221540639, 0.0758383144623013]
15919 1539 37601 2651
[0.927396, 0.06586, 0.088154, 0.077007, 0.03932038834951456, 0.1427571351642434, 0.09103876175687899]
15843 1615 37591 2661
[0.925905, 0.066109, 0.092508, 0.079308, 0.04119267459062388, 0.14380674448767833, 0.0924997095391511]
15309 2149 38937 1315
[0.939976, 0.032669, 0.123095, 0.077882, 0.05230492138441318, 0.07910250240615976, 0.06570371189528647]
15476 1982 38174 2078
[0.929648, 0.051625, 0.11353, 0.082577, 0.04935750572766212, 0.11837757776005468, 0.08386754174385841]
15765 1693 38226 2026
[0.935557, 0.050333, 0.096976, 0.073654, 0.04241088203612315, 0.11387780338373335, 0.07814434270992825]
15026 2432 38731 1521


78.42  81











multimmd
patch_kd_multiMMD_avg_spp_multi_multi_rgb_lr_0.001_version_1_weight_patch_0.pth
withoud weight:
0:[0.942003, 0.046134, 0.085348, 0.065741, 0.03735740253228031, 0.10417952314165498, 0.07076846283696764]
1:[0.93145, 0.058978, 0.090617, 0.074798, 0.04009123162696401, 0.13008219178082192, 0.08508671170389297]




with weigit_patch
4:[0.938122, 0.034682, 0.124585, 0.079633, 0.05300870073846604, 0.08369806343305954, 0.06835338208576279]
5:[0.934396, 0.042631, 0.11857, 0.080601, 0.05097768802640004, 0.10032740879326474, 0.07565254840983239]

6:[0.936666, 0.033837, 0.131344, 0.08259, 0.05567831386737246, 0.08241060083499728, 0.06904445735118486]
7:[0.932646, 0.043153, 0.123153, 0.083153, 0.05287101930406984, 0.101906717512467, 0.07738886840826842]









multikd+scale-selector
patch_kd_multiKD_select_avg_spp_multi_multi_rgb_lr_0.001_version_x_weight_patch_0
loss--linear(21,21)

without weight_patch
0:[0.94332, 0.03098, 0.115935, 0.073458, 0.049330961027565866, 0.07475571008932318, 0.06204333555844452]
1:[0.932473, 0.050904, 0.105854, 0.078379, 0.046141170008239495, 0.116031485361572, 0.08108632768490574]
2:[0.930289, 0.06822, 0.073147, 0.070684, 0.032926797823788774, 0.14508374280128916, 0.08900527031253896]
3:[0.93528, 0.045836, 0.10826, 0.077048, 0.046901754473037696, 0.10595532073738012, 0.0764285376052089]








teacher_feature_patch_strength + softmax
不行,效果很差
15768 1690 37528 2724
[0.923514, 0.067674, 0.096804, 0.082239, 0.04309245754500485, 0.1473069435431538, 0.09519970054407932]
15946 1512 37391 2861
[0.924225, 0.071077, 0.086608, 0.078843, 0.03886589723157597, 0.15212420907109056, 0.09549505315133326]
16312 1146 35682 4570
[0.900953, 0.113535, 0.065643, 0.089589, 0.03111762789182144, 0.21884876927497365, 0.12498319858339754]
14934 2524 38676 1576
[0.928955, 0.039153, 0.144576, 0.091864, 0.061262135922330097, 0.09545729860690491, 0.0783597172646175]



teacher_feature_patch_strength + nn.linear
更差



emm,多次实验证明,直接使用teacher_feature 作为加权是不行了.


student_feature_patch_strength + nn.linear
15628 1830 37865 2387
[0.926928, 0.059301, 0.104823, 0.082062, 0.046101524121425874, 0.1325006938662226, 0.08930110899382424]
15525 1933 38482 1770
[0.935834, 0.043973, 0.110723, 0.077348, 0.04782877644438946, 0.1023417172593235, 0.07508524685185648]
15478 1980 38682 1570
[0.938486, 0.039004, 0.113415, 0.07621, 0.04869411243913236, 0.09209291412482402, 0.0703935132819782]
15816 1642 38004 2248
[0.932594, 0.055848, 0.094054, 0.074951, 0.04141653634666801, 0.12444641275465013, 0.08293147455065907]



student_feature_patch_strength + mmd loss for single batch + nn.linear
name patch_kd_multiKD_mmdloss_select_avg_spp

15559 1899 38333 1919
[0.933842, 0.047675, 0.108775, 0.078225, 0.04720123284947306, 0.10979517107220506, 0.07849820196083906]
15496 1962 38019 2233
[0.927309, 0.055476, 0.112384, 0.08393, 0.049073309822165526, 0.12595183033448024, 0.08751257007832289]
15368 2090 38525 1727
[0.933859, 0.042905, 0.119716, 0.08131, 0.05145882063277114, 0.10102369113775958, 0.07624125588526537]
15541 1917 38546 1706
[0.937221, 0.042383, 0.109806, 0.076095, 0.04737661567357833, 0.09891575346437062, 0.07314618456897448]
15923 1535 37869 2383
[0.932109, 0.059202, 0.087925, 0.073564, 0.03895543599634555, 0.13017589861247678, 0.08456566730441116]
15394 2064 37361 2891
[0.930428, 0.051625, 0.110952, 0.081288, 0.04829099249582409, 0.11807489061878516, 0.08318294155730463]

78.66  8016



21 mean 系数是21个,而不是一个,也就是 21x1 1x21 相乘
self attention
pretrain_dir = "../output/models/surf_patch_kd_multikd_avg_self_attention_21_multi_multi_rgb_lr_0.001_version_" + str(
            i) + "_lambda_kd_0.5_weight_patch_1_hter_best_.pth"

hter_best
16060 1398 37487 2765
[0.927863, 0.068692, 0.080078, 0.074385, 0.035952166645235956, 0.14687915006640107, 0.09141565835581851]
16308 1150 36799 3453
[0.920239, 0.085785, 0.065872, 0.075828, 0.030303828822893883, 0.17473812054045848, 0.10252097468167617]
16161 1297 37351 2901
[0.927257, 0.072071, 0.074293, 0.073182, 0.03355930449182364, 0.15218759836323575, 0.0928734514275297]
16396 1062 36267 3985
[0.912545, 0.099001, 0.060832, 0.079917, 0.028449730772321786, 0.19552524409989697, 0.11198748743610938]
15683 1775 37973 2279
[0.929752, 0.056618, 0.101673, 0.079145, 0.04465633490993257, 0.1268789667074936, 0.08576765080871308]
15999 1459 36913 3339
[0.91686, 0.082952, 0.083572, 0.083262, 0.03802251641822162, 0.17266521874030408, 0.10534386757926285]
16186 1272 37729 2523
[0.93424, 0.06268, 0.072861, 0.06777, 0.032614548344914236, 0.13485488267678658, 0.0837347155108504]
15876 1582 37871 2381
[0.931329, 0.059152, 0.090617, 0.074885, 0.04009834486604314, 0.13041573095251136, 0.08525703790927724]


0.059,0.090, 0.743



self attention
pretrain_dir = "../output/models/surf_patch_kd_multikd_avg_self_attention_21_multi_multi_rgb_lr_0.001_version_" + str(
            i) + "_lambda_kd_1.0_weight_patch_1_hter_best_.pth"
16178 1280 37079 3173
[0.922838, 0.078828, 0.073319, 0.076074, 0.033368961651763604, 0.16397085421942018, 0.09866990793559188]
15856 1602 37824 2428
[0.930168, 0.06032, 0.091763, 0.076042, 0.040633084766397806, 0.13279369940931962, 0.08671339208785872]
15914 1544 37289 2963
[0.921903, 0.073611, 0.088441, 0.081026, 0.03975999793989648, 0.15696350055623245, 0.09836174924806447]
15822 1636 37374 2878
[0.921781, 0.0715, 0.093711, 0.082605, 0.041937964624455266, 0.15390374331550802, 0.09792085396998164]
16089 1369 37581 2671
[0.929995, 0.066357, 0.078417, 0.072387, 0.03514762516046213, 0.1423773987206823, 0.08876251194057222]
16029 1429 36850 3402
[0.916288, 0.084518, 0.081854, 0.083186, 0.03733117375062044, 0.17508105604446503, 0.10620611489754274]
15929 1529 36216 4036
[0.90357, 0.100268, 0.087582, 0.093925, 0.040508676645913365, 0.20215376909591787, 0.12133122287091562]
15723 1735 38239 2013
[0.935055, 0.05001, 0.099381, 0.074696, 0.04340321208785711, 0.1134979702300406, 0.07845059115894885]






pretrain_dir = "../output/models/surf_patch_kd_multikd_avg_self_attention_multi_multi_rgb_lr_0.001_version_" + str(
            i) + "_lambda_kd_1.0_weight_patch_1_hter_best_.pth"


15637 1821 37558 2694
[0.921764, 0.066928, 0.104307, 0.085618, 0.04624292135402118, 0.14696415907479135, 0.09660354021440626]
15662 1796 37790 2462
[0.926217, 0.061165, 0.102875, 0.08202, 0.0453695751023089, 0.13584197748841315, 0.09060577629536103]
16000 1458 37808 2444
[0.932386, 0.060717, 0.083515, 0.072116, 0.03713136046452402, 0.13250921708956842, 0.08482028877704623]
15865 1593 37606 2646
[0.926547, 0.065736, 0.091248, 0.078492, 0.04063879180591341, 0.14294203446599318, 0.09179041313595329]
16267 1191 36879 3373
[0.920915, 0.083797, 0.068221, 0.076009, 0.031284475965327026, 0.17174134419551934, 0.10151291008042318]
15627 1831 38123 2129
[0.931381, 0.052892, 0.10488, 0.078886, 0.04582770185713571, 0.11990313133588647, 0.08286541659651109]
15605 1853 38542 1710
[0.93826, 0.042482, 0.10614, 0.074311, 0.04587201386310187, 0.09875830205024545, 0.07231515795667366



pretrain_dir = "../output/models/surf_patch_kd_multikd_avg_self_attention_multi_multi_rgb_lr_0.001_version_" + str(
            i) + "_lambda_kd_1.0_weight_patch_1_hter_best_.pth"
hter
16013 1445 37501 2751
[0.927292, 0.068344, 0.08277, 0.075557, 0.03710265495814718, 0.1466105308036666, 0.09185659288090689]
15997 1461 37596 2656
[0.928661, 0.065984, 0.083687, 0.074835, 0.03740686688685767, 0.1423899640808449, 0.08989841548385127]
15937 1521 37311 2941
[0.922682, 0.073065, 0.087123, 0.080094, 0.03916872682323857, 0.15578980824239855, 0.09747926753281856]
16137 1321 37125 3127
[0.922925, 0.077686, 0.075667, 0.076676, 0.0343598813920824, 0.1623235049833887, 0.09834169318773556]
14545 2913 39382 870
[0.934448, 0.021614, 0.166858, 0.094236, 0.06887338928951413, 0.056438533895556274, 0.0626559615925352]
15977 1481 36742 3510
[0.913516, 0.087201, 0.084832, 0.086016, 0.038746304581011436, 0.1801200800533689, 0.10943319231719018]
15615 1843 38144 2108
[0.931537, 0.05237, 0.105568, 0.078969, 0.04608997924325406, 0.11894148846132145, 0.08251573385228775]



pretrain_dir = "../output/models/surf_patch_kd_multikd_avg_self_attention_21_multi_multi_rgb_lr_0.001_version_" + str(
            i) + "_lambda_kd_0.5_weight_patch_1_hter_best_.pth"

weight=teacher_patch_strength???,也没有把,我的q还是从x上来的.....那就这样吧


15866 1592 38021 2231
[0.933755, 0.055426, 0.09119, 0.073308, 0.040188826900260016, 0.12328010167431067, 0.08173446428728534]
15636 1822 38010 2242
[0.929579, 0.055699, 0.104365, 0.080032, 0.04574211689094196, 0.12540552634522878, 0.08557382161808537]
16036 1422 37404 2848
[0.926009, 0.070754, 0.081453, 0.076103, 0.03662494204914233, 0.15081550518957848, 0.0937202236193604]
16224 1234 37157 3095
[0.924987, 0.076891, 0.070684, 0.073787, 0.0321429501706129, 0.16020497955380714, 0.09617396486221003]
15874 1584 37791 2461
[0.929908, 0.06114, 0.090732, 0.075936, 0.040228571428571426, 0.1342241614398691, 0.08722636643422026]
15806 1652 37717 2535
[0.927448, 0.062978, 0.094627, 0.078803, 0.04196194975742335, 0.1382149283027098, 0.09008843903006657]
15694 1764 37968 2284
[0.929856, 0.056743, 0.101043, 0.078893, 0.04439746300211417, 0.12704416509066638, 0.08572081404639027]
16258 1200 37397 2855
[0.929735, 0.070928, 0.068736, 0.069832, 0.03109049926160064, 0.1493747710982054, 0.09023263517990303]

0.933755, 0.055426, 0.09119, 0.073308




pretrain_dir = "../output/models/surf_patch_kd_multikd_avg_self_attention_21_scale_2_multi_multi_rgb_lr_0.001_version_" + str(
            i) + "_lambda_kd_0.5_weight_patch_1_hter_best_.pth"
 去掉了最小粒度之后的结果,效果有些下降
15724 1734 37900 2352
[0.929198, 0.058432, 0.099324, 0.078878, 0.043750315385779885, 0.1301172825846426, 0.08693379898521125]
15812 1646 37552 2700
[0.924692, 0.067077, 0.094283, 0.08068, 0.04199193836420226, 0.14585133967156438, 0.09392163901788332]
15827 1631 37459 2793
[0.923341, 0.069388, 0.093424, 0.081406, 0.04172422614479406, 0.15, 0.09586211307239703]
15792 1666 38283 1969
[0.937013, 0.048917, 0.095429, 0.072173, 0.04170317154371824, 0.11086087495073475, 0.0762820232472265]
16063 1395 36754 3498
[0.915214, 0.086903, 0.079906, 0.083404, 0.036567144617159035, 0.17882521343489596, 0.1076961790260275]
16007 1451 37653 2599
[0.929822, 0.064568, 0.083114, 0.073841, 0.03710617839607201, 0.13968612275610018, 0.0883961505760861]
16046 1412 37624 2628
[0.929995, 0.065289, 0.08088, 0.073084, 0.036171738907674965, 0.14073042733211952, 0.08845108311989724]
16288 1170 36093 4159
[0.907659, 0.103324, 0.067018, 0.085171, 0.03139843812897512, 0.203403922335795, 0.11740118023238505]

avg
[0.937013, 0.053917, 0.010429, 0.077173



###################################开始分析feature的迁移了##########################################


KD+multi_rgb+random init+logit+sp_feature:
名字: patch_feature_kd_mmd_avg_sp_multi_multi_rgb_lr_0.001_version_x_sift_0.pth
[0.921833, 0.059301, 0.121663, 0.090482, 0.05311460651679212, 0.13469894475481067, 0.0939067756358014]
[0.92769, 0.053886, 0.11479, 0.084338, 0.049991268989946866, 0.12307779606196448, 0.08653453252595567]

warm up
5:[0.932022, 0.049389, 0.110837, 0.080113, 0.04813552575934725, 0.1135286391411113, 0.08083208245022927]
6:[0.937255, 0.057289, 0.075324, 0.066306, 0.03349379791650747, 0.12499322456501707, 0.07924351124076227]
7:[0.942818, 0.050159, 0.073376, 0.061768, 0.03241888950751632, 0.11095845240712245, 0.07168867095731939]
8:[0.926547, 0.070506, 0.08025, 0.075378, 0.03609429344325647, 0.15019846520243452, 0.0931463793228455]






#######################################AP+SP_init#############################################
标志是init_sp
分别测试了不同的初始化模型

KD+multi_rgb+random init+patch + mmd + avg+ AT: epoch 50 with sp init
init with patch_feature_kd_mmd_avg_sp_multi_multi_rgb_lr_0.001_version_6_sift_0.pth

4:[0.945521, 0.030458, 0.109864, 0.070161, 0.046844470496287614, 0.0731241798878683, 0.059984325192077956]
5:[0.941535, 0.030955, 0.121893, 0.076424, 0.05173335926484174, 0.07516891891891891, 0.06345113909188033]
hter_best:[0.936198, 0.053761, 0.086952, 0.070356, 0.03832752613240418, 0.11953159522757402, 0.0789295606799891]

6:[0.944048, 0.035352, 0.103448, 0.0694, 0.044444444444444446, 0.08333821376281113, 0.06389132910362778]=hter_best
    acer_best:[0.939525, 0.020024, 0.15374, 0.086882, 0.06370757180156658, 0.05173299101412067, 0.057720281407843625]

7:[0.942558, 0.032793, 0.114274, 0.073534, 0.04874532704571554, 0.07865101590895549, 0.06369817147733552]








init with patch_feature_kd_mmd_avg_sp_multi_multi_rgb_lr_0.001_version_7_sift_0.pth
name:patch_kd_mmd_avg_init_sp_multi_multi_rgb_lr_0.0001_version_7_weight_patch_True.pth
fc-random:
0:[0.937515, 0.020397, 0.159526, 0.089961, 0.0659702482471101, 0.05298825351749064, 0.05947925088230037]
1:[0.941709, 0.027874, 0.128422, 0.078148, 0.05419124045247994, 0.06867425633492472, 0.06143274839370233]
2:[0.940028, 0.0199, 0.152366, 0.086133, 0.06316639357887488, 0.051349445477274186, 0.05725791952807453]
3:[0.939092, 0.020595, 0.153855, 0.087225, 0.06378683891804603, 0.05313761938337286, 0.05846222915070944]


fc-multi:
4:0.945833, 0.023676, 0.12447, 0.074073, 0.052396797839506175, 0.05868949378002217, 0.055543145809764174]
hter_best:[0.938226, 0.060742, 0.064154, 0.062448, 0.02877180363244021, 0.13017089921737743, 0.07947135142490883]
5:[0.93878, 0.036793, 0.117539, 0.077166, 0.05026578154471744, 0.0877005980932078, 0.06898318981896262]
hter_best:[0.938589, 0.051774, 0.083629, 0.067702, 0.036842636519632585, 0.1152527375290344, 0.07604768702433348]
6:[0.942454, 0.032595, 0.115076, 0.073835, 0.04906102713130968, 0.07827695244913788, 0.06366898979022378]
hter_best:[0.941466, 0.048097, 0.082598, 0.065348, 0.03626943005181347, 0.10784313725490197, 0.07205628365335771]


对于sp init 来说, fc-multi init 相比域fc-random init 具有明显的收敛优势.无论哪一个指标都是如此.
特别的,还打破了最优hter出现在最优acc里面的情况.可以出现更好的hter结果
更加重要的是,可能不需要限定范围大于多少epoch才去保存模型了,因为在开始的时候也有可能约到很好的模型




init with patch_feature_kd_mmd_avg_sp_multi_multi_rgb_lr_0.001_version_8_sift_0.pth
6:hter_best:[0.933564, 0.041613, 0.123668, 0.082641, 0.05299980361351139, 0.09868033462943325, 0.07584006912147231]
:acer_best:[0.930549, 0.027551, 0.166056, 0.096804, 0.06895485466914038, 0.07078121010977789, 0.06986803238945913]
7:
hter_best[0.937394, 0.03883, 0.117425, 0.078128, 0.0503203318687253, 0.0920982853102351, 0.0712093085894802]
:acer_best[0.931658, 0.024148, 0.170237, 0.097193, 0.0703398655684938, 0.06288006210376504, 0.06660996383612941]





name:patch_kd_bce_avg_init_sp_multi_multi_rgb_lr_0.0001_version_7_weight_patch_True.pth

random-fc:
0:hter_best:[0.933668, 0.064643, 0.070226, 0.067434, 0.031536166272250234, 0.13815440161410217, 0.0848452839431762]
1:hter_best:[0.928868, 0.07535, 0.061405, 0.068377, 0.027996134861978012, 0.15618723930171483, 0.09209168708184642]


multi-fc:
4:hter_best:[0.930515, 0.072866, 0.061691, 0.067278, 0.028049796853838942, 0.15185875530703116, 0.08995427608043505]
5:hter_best:[0.929839, 0.074928, 0.059171, 0.067049, 0.026993127596749326, 0.15513605267218764, 0.09106459013446848]






#########################研究SP_INIT 结合参数冻结##########
冻结是指冻结SP的特征,只处理fc的特征.

init with patch_feature_kd_mmd_max_avg_multi_multi_rgb_lr_0.001_version_7_sift_0.pth
name:patch_kd_bce_avg_init_sp_multi_multi_rgb_lr_0.0001_version_7_weight_patch_True_freeze.pth
free sp 参数,调整fc 参数.
freeze and bceloss

random-fc:
0:[0.946543, 0.039551, 0.08552, 0.062535, 0.03718277588225039, 0.09067608361337358, 0.06392942974781199]
hter_best:[0.938849, 0.06027, 0.06318, 0.061725, 0.028333633024223585, 0.12917310047388317, 0.07875336674905338]

1:[0.94131, 0.056593, 0.063524, 0.060059, 0.028375508533121818, 0.12229559241960594, 0.07533555047636388]

multi-fc:
4:[0.937203, 0.061463, 0.065872, 0.063668, 0.029541718043567613, 0.13172186135661804, 0.08063178970009283]
hter_best:[0.937203, 0.061463, 0.065872, 0.063668, 0.029541718043567613, 0.13172186135661804, 0.08063178970009283]
acer_best:[0.942055, 0.033067, 0.115305, 0.074186, 0.04917672350613182, 0.07933953266571292, 0.06425812808592238]

5:[0.937532, 0.060792, 0.066331, 0.063561, 0.029720504067961912, 0.13052755107483865, 0.08012402757140029]
hter_best:


freez 相比不freeze,效果更加的好
freeze 之后random fc的效果更好


init with patch_feature_kd_mmd_avg_sp_multi_multi_rgb_lr_0.001_version_8_sift_0.pth
freeze
bceloss

    random-fc
    0:[0.931069, 0.043178, 0.128308, 0.085743, 0.05496392992098935, 0.10250058976173626, 0.07873225984136281]
    hter_best:[0.931069, 0.043178, 0.128308, 0.085743, 0.05496392992098935, 0.10250058976173626, 0.07873225984136281]

    1:[0.930982, 0.045215, 0.123897, 0.084556, 0.05328242394383545, 0.10633946830265849, 0.07981094612324696]
    hter_best:[0.937394, 0.03883, 0.117425, 0.078128, 0.0503203318687253, 0.0920982853102351, 0.0712093085894802]
    acer_best:[0.931658, 0.024148, 0.170237, 0.097193, 0.0703398655684938, 0.06288006210376504, 0.06660996383612941]

    multi-fc
    4:[0.915127, 0.08648, 0.081166, 0.083823, 0.03710589714046297, 0.17831164839668068, 0.10770877276857183]


freeze
mmd_loss
    random-fc
    1:[0.935003, 0.031949, 0.141196, 0.086572, 0.059496512273418456, 0.0789974814177775, 0.06924699684559799]
    hter_best:[0.932577, 0.042358, 0.125215, 0.083786, 0.05366656028281737, 0.10042999352064558, 0.07704827690173147]
    2:[0.932854, 0.04129, 0.126761, 0.084026, 0.05423620812195182, 0.09830247826344118, 0.0762693431926965]


整个AP+SP_INIT看起来效果似乎是比单个的AP有提升,但是最后却还甚至比不上,emmm,单独的SP吧,也就是提升是SP带来的,到这里一切都是对的,但是还比不上单独SP这个怎么说?
首先AP本身还有一个损失函数可以调节,这个可以帮提高效果,控制效果









init with patch_feature_kd_mmd_max_avg_multi_multi_rgb_lr_0.001_version_5_sift_0.pth
name:patch_kd_mmd_avg_init_mse_multi_multi_rgb_lr_0.0001_version_7_weight_patch_True.pth
random fc
0:[0.923826, 0.069487, 0.091591, 0.080539, 0.04094330926409587, 0.14992495711835335, 0.09543413319122461]
1:[0.923601, 0.070059, 0.091018, 0.080539, 0.04072166269444658, 0.15089089838942693, 0.09580628054193675]
2:[0.913602, 0.089114, 0.080135, 0.084624, 0.03675388818831442, 0.18258169601954596, 0.10966779210393018]
3:[0.917813, 0.077561, 0.092851, 0.085206, 0.04183117855023096, 0.1646711324436943, 0.10325115549696263]

multi-fc
5:[0.915474, 0.085983, 0.081166, 0.083575, 0.03708647403685092, 0.17746897754076504, 0.10727772578880798]
6:[0.910778, 0.09371, 0.078875, 0.086292, 0.036373722164989304, 0.18999647408452122, 0.11318509812475526]
7:[0.907364, 0.100268, 0.075037, 0.087653, 0.0349091296700954, 0.19996036464526357, 0.11743474715767949]

mse 初始化,random和fc的效果都挺差的.





############################sp init with spp##########################

emmmm, 好像也不是就很有优势?是好一点,但是没有说好上很多





#########################################集成###################################

sp_8+ap_3 集成
0:[0.939421, 0.053289, 0.077386, 0.065338, 0.034239, 0.117521, 0.07588]
1:[0.937862, 0.057985, 0.071715, 0.06485, 0.031963, 0.12589, 0.078927]
2:[0.939958, 0.052047, 0.078474, 0.065261, 0.03466, 0.115218, 0.074939]
3:

warm up
5:[0.940062, 0.052768, 0.076469, 0.064618, 0.033829, 0.116403, 0.075116]


analysi: 真的有提升!!!,比两个模型都好,对初始化条件降低了.

sp_6+ap_2 两个效果更好的基础模型
0:[0.947669, 0.043203, 0.073376, 0.058289, 0.032191, 0.097064, 0.064627]
1:[0.948986, 0.040694, 0.074808, 0.057751, 0.032715, 0.092074, 0.062395]
2:[0.948692, 0.04042, 0.076412, 0.058416, 0.033384, 0.091657, 0.062521]
3:[0.94682, 0.04596, 0.069825, 0.057893, 0.030767, 0.102272, 0.066519]

warm up
4:[0.947357, 0.042905, 0.075095, 0.059, 0.03291, 0.096621, 0.064765]
5:[0.947583, 0.043054, 0.074006, 0.05853, 0.032453, 0.096821, 0.064637]
6:[0.948657, 0.039501, 0.078646, 0.059074, 0.034295, 0.089958, 0.062126]
7:[0.948848, 0.039427, 0.078188, 0.058807, 0.034099, 0.089762, 0.061931]


sp_7+ap_0 acer_best
name: sp_ensemble_ap_acer_best
acer_best:[0.949835, 0.034508, 0.086264, 0.060386, 0.037306, 0.080099, 0.058703]
acer_best:[0.950061, 0.033911, 0.086894, 0.060403, 0.037546, 0.078874, 0.05821]
基本就是在这附近了.难以为继,而且其实也很接近了,原始模型的精度


multi-kd
best ssp
parser.add_argument('--sp_model', type=str,
                    default='patch_feature_kd_mmd_avg_sp_multi_multi_rgb_lr_0.001_version_7_sift_0_acer_best_.pth')
parser.add_argument('--sp_freeze', type=int, default=1)
parser.add_argument('--ap_model', type=str,
                    default='patch_kd_multiKD_multi_multi_rgb_lr_0.001_version_5_weight_patch_0_select_0_acer_best_.pth'


save:sp_ensemble_ap_multiKD_acer_best__lr_0.0001_version_0_acer_best_.pth     sp_ensemble_ap_multiKD_acer_best__lr_0.0001_version_1_acer_best_.pth
normal
[0.945642, 0.045613, 0.074522, 0.060067, 0.032757, 0.10204, 0.067398]
[0.945711, 0.045588, 0.07435, 0.059969, 0.032683, 0.101973, 0.067328]


acer_best
[0.948969, 0.037066, 0.083228, 0.060147, 0.036133, 0.085272, 0.060702]
[0.950165, 0.036023, 0.081682, 0.058852, 0.035448, 0.082942, 0.059195]




multikd
worst spp

[0.947063, 0.033737, 0.097205, 0.065471, 0.041807, 0.079327, 0.060567]



ganloss
[0.920724, 0.038532, 0.173216, 0.105874, 0.07247453565008988, 0.0970284641851736]
[0.906914, 0.116367, 0.039409, 0.077888, 0.018976169461606355, 0.21832758459960847]
[0.934067, 0.044072, 0.116336, 0.080204, 0.050137006591127894, 0.10313353874774722]
1:[0.921244, 0.071574, 0.095314, 0.083444, 0.04262841040092225, 0.15427041499330657]
2:[0.906411, 0.101709, 0.074865, 0.088287, 0.03488589350060056, 0.2022227710545814]
2:[0.921573, 0.077363, 0.08088, 0.079121, 0.03662775616083009, 0.16252609603340293]
#depth 模态的转移,适合lr=1e-4,而ir似乎更加适合lr=1e-3
#训练不稳定,可能需要多次训练.


CROSS REPLAYE:
ap:[0.545249, 0.468501, 0.4289, 0.448701, 0.30032910860583995, 0.6066443199559188, 0.4534867142808794]
sp:[0.368488, 0.83154, 0.255467, 0.543503, 0.44648995048062917, 0.6773839443048213, 0.5619369473927253]
ap+sp:[0.403107, 0.767159, 0.2768, 0.521979, 0.3873857062884867, 0.6660252759262965, 0.5267054911073916]





##########################CEFA#################################
multimodal
0:[0.738344, 0.390225, 0.190997, 0.290611, 0.3630296782594542, 0.20954426284375524] 28.62
1:[0.713217, 0.301927, 0.27846, 0.290193, 0.42056477689157146, 0.18697330331296236]
2:[0.776365, 0.379058, 0.138216, 0.258637, 0.2882657617431684, 0.19467562953478446]  24.1



########################depth############################
0:[0.529631, 0.045344, 0.703956, 0.37465, 0.572965, 0.077641]
1:[0.555347, 0.092817, 0.638017, 0.365417, 0.561343, 0.123514]
2:[0.529631, 0.045344, 0.703956, 0.37465, 0.572965, 0.077641]
3:[0.533149, 0.044149, 0.699161, 0.371655, 0.570986, 0.074633]


#####################ir##################
0:[0.416008, 0.000623, 0.904602, 0.452612, 0.622214, 0.003578]
1:[0.416008, 0.000623, 0.904602, 0.452612, 0.622214, 0.003578]
2:




#################profile########################
1:[0.64049, 0.308108, 0.38776, 0.347934, 0.504888, 0.216654]
2:


##########################spp##############
cefa_resnet18_no_dropout_no_seed_no_share_multi_version_2.pth:
1:[0.744699, 0.404197, 0.17347, 0.288833, 0.3463072714839298, 0.21183003511446225, 0.27906865329919606]
2:[0.734715, 0.384148, 0.19996, 0.292054, 0.3713816138267416, 0.20879089856872654, 0.2900862561977341]
3:[0.757907, 0.289565, 0.216003, 0.252784, 0.3561779242174629, 0.16873486682808717, 0.26245639552277505]
4:[0.732302, 0.381915, 0.204927, 0.293421, 0.37627758268253053, 0.20885644492416067, 0.2925670138033456]


cefa_resnet18_no_dropout_no_seed_no_share_multi_version_0.pth:

spp:[0.757907, 0.289565, 0.216003, 0.252784, 0.3561779242174629, 0.16873486682808717, 0.26245639552277505]
mse:[0.726978, 0.474368, 0.162366, 0.318367, 0.3598178137651822, 0.23736258024274243, 0.2985901970039623]
at:[0.747702, 0.362022, 0.191996, 0.277009, 0.35383239518122994, 0.1975847601768908, 0.2757085776790604],[0.720162, 0.409806, 0.208409, 0.309108, 0.39118088298328335, 0.22149855422363213, 0.30633971860345777]





#################ap#########################

cefa_resnet18_no_dropout_no_seed_no_share_multi_version_2.pth
0:[0.781339, 0.306394, 0.170444, 0.238419, 0.3089780077619664, 0.16873569794050344, 0.23885685285123492]
2:[0.762992, 0.256947, 0.22605, 0.241499, 0.35631046119235094, 0.15430442919525889, 0.2553074451938049]
3:[0.803776, 0.271334, 0.154944, 0.213139, 0.2789741481214987, 0.14999425749397036, 0.2144842028077345]
4:[0.756434, 0.344466, 0.188114, 0.26629, 0.34303263755140284, 0.18908593259964646, 0.2660592850755247]
5:[0.793774, 0.336986, 0.134363, 0.235675, 0.2694024725274725, 0.17624208839268737, 0.22282228046007996]
7:[0.76966, 0.248117, 0.22057, 0.234343, 0.3480160338692969, 0.1488996945327598, 0.24845786420102836]


cefa_resnet18_no_dropout_no_seed_no_share_multi_version_0.pth
multikd:[0.732357, 0.308939, 0.244947, 0.276943, 0.3920771269304578, 0.18358591314546746, 0.28783152003796264]
kd:[0.693635, 0.242144, 0.341659, 0.291902, 0.45064006024096387, 0.16815148782687106, 0.3093957740339175]
spp_max: [0.774615, 0.312627, 0.177438, 0.245032, 0.31958868894601544, 0.17278599110090426, 0.24618734002345985],[0.769697, 0.310913, 0.186001, 0.248457, 0.32937370469595106, 0.17349718856877863, 0.25143544663236483]
spp_max: [0.782481, 0.284008, 0.180977, 0.232493, 0.31503105590062114, 0.16007025761124122, 0.23755065675593118],[0.751349, 0.369137, 0.182433, 0.275785, 0.344769919620219, 0.19880832494125544, 0.27178912228073726]






####################ensemble######################################

23.02

[0.757263, 0.326391, 0.196763, 0.261577, 0.347045, 0.182552, 0.264798]
[0.758865, 0.35501, 0.178551, 0.26678, 0.334976, 0.19193, 0.263453]


#### 敏感性实验 on CASIA-SURF###############
scale-distill
0.2
0.77±0.2
15968 1490 37694 2558
[0.929856, 0.06355, 0.085348, 0.074449, 0.03802572478562679, 0.13807621720824786, 0.08805097099693733]
15431 2027 38386 1866
[0.932542, 0.046358, 0.116107, 0.081233, 0.05015712765694207, 0.10787997918714227, 0.07901855342204217]
14210 3248 39053 1199
[0.922942, 0.029787, 0.186047, 0.107917, 0.07678305477411881, 0.07781166850541892, 0.07729736163976886]
13282 4176 39198 1054
[0.909374, 0.026185, 0.239203, 0.132694, 0.09627887674643795, 0.07352120535714286, 0.08490004105179041]


7.8±0.4
16170 1288 36842 3410
[0.918593, 0.084716, 0.073777, 0.079247, 0.03377917650144244, 0.17415730337078653, 0.10396823993611448]
15933 1525 37893 2359
[0.932698, 0.058606, 0.087353, 0.072979, 0.03868790907707139, 0.12896348130330199, 0.08382569519018669]
15745 1713 38239 2013
[0.935436, 0.05001, 0.098121, 0.074066, 0.04287645174209051, 0.11335736006307016, 0.07811690590258033]



0.73±0.4
[0.934206, 0.03411, 0.138848, 0.086479, 0.058688230879112895, 0.0836837935027732, 0.07118601219094305]
14974 2484 38530 1722
[0.927118, 0.04278, 0.142284, 0.092532, 0.060564685229433854, 0.10313847628174413, 0.08185158075558899]
16028 1430 36310 3942
[0.906914, 0.097933, 0.081911, 0.089922, 0.037890832008479064, 0.19739609414121181, 0.11764346307484544]
15431 2027 38224 2028
[0.929735, 0.050383, 0.116107, 0.083245, 0.050358997291992745, 0.11615785554728221, 0.08325842641963747]

0.76±0.3
15349 2109 38098 2154
[0.926131, 0.053513, 0.120804, 0.087159, 0.052453552863929165, 0.12306461749414387, 0.08775908517903652]
14736 2722 38889 1363
[0.929215, 0.033862, 0.155917, 0.094889, 0.06541539496767682, 0.08466364370457792, 0.07503951933612737]
15168 2290 38257 1995
[0.925749, 0.049563, 0.131172, 0.090367, 0.05647766789158261, 0.11623841985666841, 0.08635804387412552]




diff

1:
9.1±0.3
[0.909565, 0.096368, 0.076756, 0.086562, 0.035531514331927985, 0.1939790968645297, 0.11475530559822883]
[0.925697, 0.068792, 0.087009, 0.0779, 0.03894672068099072, 0.14801154586273252, 0.09347913327186162]
[0.932334, 0.038011, 0.136041, 0.087026, 0.05779010633379565, 0.09209655089387829, 0.07494332861383697]




30:
6.5±0.4
[0.943961, 0.024545, 0.128652, 0.076598, 0.05410744398940014, 0.060987654320987655, 0.0575475491551939]
[0.938503, 0.037216, 0.117482, 0.077349, 0.05026344810684965, 0.0886128364389234, 0.06943814227288653]
[0.934084, 0.026359, 0.15712, 0.091739, 0.06541231458959317, 0.06725405679513184, 0.0663331856923625]

60
6.2±0.4
[0.945503, 0.031278, 0.108031, 0.069654, 0.04613615792949925, 0.07480244786406036, 0.060469302896779806]
[0.938191, 0.045091, 0.100355, 0.072723, 0.043594018263704, 0.10358997774099651, 0.07359199800235025]
[0.942055, 0.020719, 0.143774, 0.082247, 0.05986452966991032, 0.052845013306298316, 0.05635477148810432]
[0.94053, 0.034831, 0.116279, 0.075555, 0.04965753424657534, 0.08330362448009507, 0.06648057936333521]


100
6.5±0.3
[0.933408, 0.024744, 0.163077, 0.093911, 0.06761988456879557, 0.06381751778048311, 0.06571870117463935]
[0.938797, 0.035899, 0.119544, 0.077721, 0.05103438157186874, 0.08593006660323502, 0.06848222408755188]


##############CASIASURF #############
## 敏感性



0.1

15641 1817 38059 2193
[0.930515, 0.054482, 0.104078, 0.07928, 0.045566255391714314, 0.12296736570595491, 0.08426681054883461]
15439 2019 38131 2121
[0.928262, 0.052693, 0.115649, 0.084171, 0.05028642590286426, 0.12078587699316629, 0.08553615144801527]
16315 1143 35594 4658
[0.89948, 0.115721, 0.065471, 0.090596, 0.031113046792062497, 0.2220950746197492, 0.12660406070590585]
16151 1307 37017 3235
[0.921296, 0.080369, 0.074865, 0.077617, 0.03410395574574679, 0.16687300113483958, 0.10048847844029318]


0.2
[0.930567, 0.053215, 0.106828, 0.080021, 0.0466541588492808, 0.12077812235692134, 0.08371614060310106]
15267 2191 38050 2202
[0.923878, 0.054705, 0.125501, 0.090103, 0.05444695708357148, 0.12605186330070411, 0.0902494101921378]
15896 1562 37529 2723
[0.925749, 0.067649, 0.089472, 0.07856, 0.039958046609193935, 0.1462484558784038, 0.09310325124379887]
15930 1528 37332 2920
[0.922925, 0.072543, 0.087524, 0.080034, 0.0393206381883685, 0.15490716180371353, 0.09711389999604102]



0.3
15618 1840 38361 1891
[0.935349, 0.046979, 0.105396, 0.076187, 0.045770005721250714, 0.10800159917756583, 0.07688580244940826]
16521 937 36325 3927
[0.915717, 0.09756, 0.053672, 0.075616, 0.025146261606999086, 0.1920481220657277, 0.10859719183636339]
15365 2093 38295 1957
[0.929822, 0.048619, 0.119888, 0.084253, 0.051822323462414575, 0.11297771619905322, 0.0824000198307339]
15646 1812 38397 1855
[0.936458, 0.046085, 0.103792, 0.074938, 0.045064537790047, 0.10599394320324552, 0.07552924049664626]


0.4
16063 1395 37642 2610
[0.930601, 0.064841, 0.079906, 0.072374, 0.03573532802213285, 0.13977400524821934, 0.08775466663517609]
15976 1482 37695 2557
[0.930012, 0.063525, 0.084889, 0.074207, 0.03782831763534727, 0.13797010737603194, 0.0878992125056896]
15784 1674 38461 1791
[0.939958, 0.044495, 0.095887, 0.070191, 0.04170923134421328, 0.10190611664295875, 0.07180767399358601]
15840 1618 37803 2449
[0.929527, 0.060842, 0.09268, 0.076761, 0.04104411354354278, 0.13390562633276834, 0.08747486993815556]



0.5
15867 1591 36959 3293
[0.91537, 0.08181, 0.091133, 0.086471, 0.04127107652399481, 0.17186847599164926, 0.10656977625782203]
16302 1156 36212 4040
[0.909964, 0.100368, 0.066216, 0.083292, 0.03093555983729394, 0.1986038737587258, 0.11476971679800986]
15925 1533 36795 3457
[0.913533, 0.085884, 0.087811, 0.086847, 0.039996869129618036, 0.17836136621607676, 0.1091791176728474]
16292 1166 36761 3491
[0.919303, 0.086729, 0.066789, 0.076759, 0.03074326996598729, 0.17646464135874235, 0.10360395566236481]
15887 1571 37453 2799
[0.924277, 0.069537, 0.089987, 0.079762, 0.04025727757277573, 0.1497912875949909, 0.09502428258388332]
16063 1395 37094 3158
[0.921106, 0.078456, 0.079906, 0.079181, 0.036244121697108264, 0.1642994641277769, 0.10027179291244259]
15946 1512 37871 2381
[0.932542, 0.059152, 0.086608, 0.07288, 0.038392199680065, 0.12991760790091123, 0.08415490379048811]
15877 1581 37870 2382
[0.931329, 0.059177, 0.09056, 0.074869, 0.04007502978378241, 0.13045621337422642, 0.08526562157900441]




0.6
15967 1491 37007 3245
[0.917935, 0.080617, 0.085405, 0.083011, 0.03872928463816302, 0.16890485113470746, 0.10381706788643524]
16421 1037 35792 4460
[0.904748, 0.110802, 0.0594, 0.085101, 0.028157158760759183, 0.2135913030985106, 0.1208742309296349]
16073 1385 37627 2625
[0.930515, 0.065214, 0.079333, 0.072274, 0.03550189685225059, 0.14038934645416623, 0.0879456216532084]
15899 1559 37881 2371
[0.931901, 0.058904, 0.0893, 0.074102, 0.03952839756592292, 0.12977558839627806, 0.08465199298110049]




0.7
16123 1335 37201 3051
[0.923999, 0.075797, 0.076469, 0.076133, 0.03464293128503218, 0.15912172733910504, 0.09688232931206861]
15798 1660 36987 3265
[0.91466, 0.081114, 0.095085, 0.0881, 0.04295288120682071, 0.17127419608666003, 0.10711353864674036]
15448 2010 38097 2155
[0.927829, 0.053538, 0.115133, 0.084336, 0.05011593986087217, 0.12242231437823098, 0.08626912711955158]
16419 1039 35120 5132
[0.893069, 0.127497, 0.059514, 0.093506, 0.028734201720180316, 0.2381328012621224, 0.13343350149115135]

0.8
16113 1345 37839 2413
[0.934881, 0.059947, 0.077042, 0.068495, 0.034325234789710085, 0.13024937925078267, 0.08228730702024638]
16157 1301 37144 3108
[0.923601, 0.077214, 0.074522, 0.075868, 0.03384055143711796, 0.16132883467427978, 0.09758469305569886]
15813 1645 37489 2763
[0.923618, 0.068643, 0.094226, 0.081434, 0.04203505902795523, 0.14874031007751937, 0.0953876845527373]
16045 1413 37399 2853
[0.926079, 0.070878, 0.080937, 0.075908, 0.03640626610326703, 0.15096835643983492, 0.09368731127155097]



0.9
[0.930376, 0.06263, 0.085749, 0.07419, 0.038161517283572956, 0.1364029866897522, 0.08728225198666258]
16107 1351 37246 3006
[0.924502, 0.07468, 0.077386, 0.076033, 0.03500272041868539, 0.1572751530371998, 0.0961389367279426]
16161 1297 36673 3579
[0.915509, 0.088915, 0.074293, 0.081604, 0.03415854622070055, 0.18130699088145896, 0.10773276855107976]
16004 1454 36849 3403
[0.915838, 0.084542, 0.083286, 0.083914, 0.03796047306999452, 0.17534910083990313, 0.10665478695494882]
